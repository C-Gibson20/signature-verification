{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNRALezo+gGgX9pVWLYZDnb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "id": "Sry4hs0wxfDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37af9b1e-2e69-460a-a06c-24db99449aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imgaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5vr6ZqeoiQC",
        "outputId": "9d2b2426-16ad-4c8d-91ff-fd63407e17bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGhtJRpU1_XV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from scipy.optimize import brentq\n",
        "from sklearn.metrics import roc_curve\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Conv2D, Flatten, BatchNormalization, Input,\n",
        "    Add, Lambda, GlobalAveragePooling2D, Concatenate, Dropout\n",
        ")\n",
        "from tensorflow.keras.callbacks import (\n",
        "    ModelCheckpoint, EarlyStopping, LearningRateScheduler,\n",
        "    TensorBoard, ReduceLROnPlateau\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.model_selection import KFold\n",
        "import keras_tuner as kt\n",
        "import imgaug.augmenters as iaa\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "4hUuo0Bc2FpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65346188-42b1-426b-e7a1-e02cef130581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.15.0\n",
            "Running on a TPU w/8 cores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "KPBo-5IzQLOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_norm(img_path):\n",
        "\n",
        "    img_arr = cv.resize(cv.imread(img_path, cv.IMREAD_GRAYSCALE), (155, 220))\n",
        "    thresh_img = cv.threshold(img_arr, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)[1]\n",
        "    inv_thresh_img = abs(255 - thresh_img)\n",
        "\n",
        "    return inv_thresh_img\n",
        "\n",
        "\n",
        "def process_images(image_list, test_dict, train_dict, threshold=11):\n",
        "\n",
        "    for img in image_list:\n",
        "        img_id = img.split('/')[-1].split('_')[1]\n",
        "        normalized_img = img_norm(img.split(':')[-1].strip())\n",
        "        if int(img_id) <= threshold:\n",
        "            test_dict[img_id].append(normalized_img)\n",
        "        else:\n",
        "            train_dict[img_id].append(normalized_img)"
      ],
      "metadata": {
        "id": "a1pq_XJh2INF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_image(image):\n",
        "    aug = iaa.Sequential([\n",
        "        iaa.Sometimes(0.5, iaa.Affine(\n",
        "            rotate=(-5, 5),\n",
        "            scale=(0.9, 1.1),\n",
        "            translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}\n",
        "        )),\n",
        "        iaa.Sometimes(0.25, iaa.ElasticTransformation(alpha=1.0, sigma=0.25))\n",
        "    ])\n",
        "    augmented_image = aug(image=image)\n",
        "    return augmented_image\n",
        "\n",
        "\n",
        "def augment_images(images, augment=True):\n",
        "    augmented_images = []\n",
        "    for img in images:\n",
        "        img = augment_image(img)\n",
        "        augmented_images.append(img)\n",
        "    return augmented_images"
      ],
      "metadata": {
        "id": "QecMKuYYsmze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(org_dict_train, forg_dict_train, batch_size, augment=False, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    keys = list(org_dict_train.keys())\n",
        "    keys_len = len(keys)\n",
        "\n",
        "    while True:\n",
        "        anchor_images = np.zeros((batch_size, 220, 155, 3), dtype=np.float32)\n",
        "        positive_images = np.zeros((batch_size, 220, 155, 3), dtype=np.float32)\n",
        "        negative_images = np.zeros((batch_size, 220, 155, 3), dtype=np.float32)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            key = keys[np.random.randint(keys_len)]\n",
        "\n",
        "            org_images = org_dict_train[key]\n",
        "            forg_images = forg_dict_train[key]\n",
        "\n",
        "            if augment is True:\n",
        "              org_images = augment_images(org_images)\n",
        "              forg_images = augment_images(forg_images)\n",
        "\n",
        "            indices = np.random.choice(len(org_images), 2, replace=False)\n",
        "            anchor_img = np.expand_dims(org_images[indices[0]], axis=-1)\n",
        "            positive_img = np.expand_dims(org_images[indices[1]], axis=-1)\n",
        "            negative_img = np.expand_dims(forg_images[np.random.randint(len(forg_images))], axis=-1)\n",
        "\n",
        "            anchor_images[i] = np.tile(anchor_img, (1, 1, 1, 3))\n",
        "            positive_images[i] = np.tile(positive_img, (1, 1, 1, 3))\n",
        "            negative_images[i] = np.tile(negative_img, (1, 1, 1, 3))\n",
        "\n",
        "        yield anchor_images, positive_images, negative_images"
      ],
      "metadata": {
        "id": "RgCWsUSr2K9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semi_hard_triplet_loss(labels, embeddings, margin=0.2):\n",
        "\n",
        "    pairwise_distances = tf.matmul(embeddings, embeddings, transpose_b=True)\n",
        "    pairwise_distances = 1.0 - pairwise_distances\n",
        "\n",
        "    anchor_positive_dist = tf.expand_dims(pairwise_distances, 2)\n",
        "    anchor_negative_dist = tf.expand_dims(pairwise_distances, 1)\n",
        "\n",
        "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
        "\n",
        "    labels = tf.reshape(labels, [-1, 1])\n",
        "    mask_anchor_positive = tf.equal(labels, tf.transpose(labels))\n",
        "    mask_anchor_negative = tf.not_equal(labels, tf.transpose(labels))\n",
        "\n",
        "    mask_anchor_positive = tf.expand_dims(mask_anchor_positive, 2)\n",
        "    mask_anchor_negative = tf.expand_dims(mask_anchor_negative, 1)\n",
        "\n",
        "    semi_hard_triplet_mask = tf.logical_and(\n",
        "        tf.less(anchor_positive_dist, anchor_negative_dist),\n",
        "        tf.less(anchor_negative_dist, anchor_positive_dist + margin)\n",
        "    )\n",
        "\n",
        "    mask_triplets = tf.logical_and(mask_anchor_positive, mask_anchor_negative)\n",
        "    mask_triplets = tf.logical_and(mask_triplets, semi_hard_triplet_mask)\n",
        "\n",
        "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
        "    mask_triplets = tf.cast(mask_triplets, tf.float32)\n",
        "\n",
        "    triplet_loss = tf.multiply(mask_triplets, triplet_loss)\n",
        "    valid_triplet_count = tf.reduce_sum(mask_triplets)\n",
        "    triplet_loss = tf.reduce_sum(triplet_loss) / (valid_triplet_count + 1e-16)\n",
        "\n",
        "    return triplet_loss"
      ],
      "metadata": {
        "id": "Rexa_e-V2MxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_model(input_shape=(220, 155, 3), embedding_size=256):\n",
        "\n",
        "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    for layer in base_model.layers[-30:]:\n",
        "      layer.Trainable = True\n",
        "\n",
        "    intermediate_layer = base_model.get_layer('conv4_block6_add').output\n",
        "\n",
        "    x = Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(intermediate_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.01))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    skip_connection = Conv2D(64, kernel_size=1, activation='relu', padding='same')(intermediate_layer)\n",
        "    skip_connection = BatchNormalization()(skip_connection)\n",
        "\n",
        "    x = Add()([x, skip_connection])\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(embedding_size)(x)\n",
        "    x = Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(x)\n",
        "\n",
        "    return Model(inputs=base_model.input, outputs=x)"
      ],
      "metadata": {
        "id": "s6ZGq2xh2Qem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, embedding_model, margin=0.2, threshold=0.5):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding_model = embedding_model\n",
        "        self.margin = margin\n",
        "        self.threshold = threshold\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.val_accuracy_tracker = tf.keras.metrics.BinaryAccuracy(name=\"val_accuracy\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        anchor, positive, negative = inputs\n",
        "        anchor_embedding = self.embedding_model(anchor)\n",
        "        positive_embedding = self.embedding_model(positive)\n",
        "        negative_embedding = self.embedding_model(negative)\n",
        "\n",
        "        return anchor_embedding, positive_embedding, negative_embedding\n",
        "\n",
        "\n",
        "    def train_step(self, data):\n",
        "\n",
        "        anchor, positive, negative = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            anchor_embedding, positive_embedding, negative_embedding = self((anchor, positive, negative))\n",
        "\n",
        "            embeddings = tf.concat([anchor_embedding, positive_embedding, negative_embedding], axis=0)\n",
        "            labels = tf.concat([\n",
        "                tf.zeros(anchor_embedding.shape[0]),\n",
        "                tf.ones(positive_embedding.shape[0]),\n",
        "                2 * tf.ones(negative_embedding.shape[0])\n",
        "            ], axis=0)\n",
        "\n",
        "            loss = semi_hard_triplet_loss(labels, embeddings, margin=self.margin)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.embedding_model.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.embedding_model.trainable_weights))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        anchor, positive, negative = data\n",
        "        anchor_embedding, positive_embedding, negative_embedding = self((anchor, positive, negative))\n",
        "\n",
        "        anchor_embedding = tf.math.l2_normalize(anchor_embedding, axis=1)\n",
        "        positive_embedding = tf.math.l2_normalize(positive_embedding, axis=1)\n",
        "        negative_embedding = tf.math.l2_normalize(negative_embedding, axis=1)\n",
        "\n",
        "        pos_distances = 1 - tf.reduce_sum(anchor_embedding * positive_embedding, axis=1)\n",
        "        neg_distances = 1 - tf.reduce_sum(anchor_embedding * negative_embedding, axis=1)\n",
        "\n",
        "        pos_predictions = pos_distances < self.threshold\n",
        "        neg_predictions = neg_distances > self.threshold\n",
        "\n",
        "        y_true = tf.concat([tf.ones_like(pos_predictions), tf.zeros_like(neg_predictions)], axis=0)\n",
        "        y_pred = tf.concat([tf.cast(pos_predictions, tf.int32), tf.cast(neg_predictions, tf.int32)], axis=0)\n",
        "\n",
        "        self.val_accuracy_tracker.update_state(y_true, y_pred)\n",
        "\n",
        "        return {\"val_accuracy\": self.val_accuracy_tracker.result(), \"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "\n",
        "    def custom_predict(self, anchor, test):\n",
        "      anchor = img_norm(anchor)\n",
        "      test = img_norm(test)\n",
        "\n",
        "      anchor_embedding = self.embedding_model.predict(anchor)\n",
        "      test_embedding = self.embedding_model.predict(test)\n",
        "\n",
        "      similarity = tf.keras.losses.cosine_similarity(anchor_embedding, test_embedding)\n",
        "      return similarity\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        self.val_accuracy_tracker.reset_states()\n",
        "        self.loss_tracker.reset_states()\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super(CustomModel, self).get_config()\n",
        "        config.update({\n",
        "            \"embedding_model\": self.embedding_model,\n",
        "            \"margin\": self.margin,\n",
        "            \"threshold\": self.threshold\n",
        "        })\n",
        "\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.val_accuracy_tracker]"
      ],
      "metadata": {
        "id": "_4IFFnKH2SM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(embedding_size=256, margin=0.2, initial_lr=0.001, threshold=0.2):\n",
        "\n",
        "    embedding_model = create_embedding_model(input_shape=(220, 155, 3), embedding_size=embedding_size)\n",
        "\n",
        "    return CustomModel(embedding_model, margin=margin, threshold=threshold)"
      ],
      "metadata": {
        "id": "l-v2U6d42ULl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate_on_tpu(org_dict_train, forg_dict_train, kf, embedding_size=256, margin=0.2, initial_lr=0.001, threshold=0.5, epochs=25, batch_size=128, seed=42):\n",
        "    all_val_accuracies = []\n",
        "    checkpoint_paths = []\n",
        "\n",
        "    keys = list(org_dict_train.keys())\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(keys)):\n",
        "        train_keys = [keys[i] for i in train_index]\n",
        "        val_keys = [keys[i] for i in val_index]\n",
        "\n",
        "        train_org_dict = {key: org_dict_train[key] for key in train_keys}\n",
        "        val_org_dict = {key: org_dict_train[key] for key in val_keys}\n",
        "\n",
        "        train_forg_dict = {key: forg_dict_train[key] for key in train_keys}\n",
        "        val_forg_dict = {key: forg_dict_train[key] for key in val_keys}\n",
        "\n",
        "        with tpu_strategy.scope():\n",
        "            model = create_model(embedding_size=embedding_size, margin=margin, initial_lr=initial_lr, threshold=threshold)\n",
        "            steps_per_epoch = (len(train_org_dict) * 48) // batch_size\n",
        "            validation_steps = (len(val_org_dict) * 48) // batch_size\n",
        "\n",
        "            optimizer = Adam(learning_rate=initial_lr)\n",
        "            model.compile(optimizer, weighted_metrics=[])\n",
        "            model.reset_metrics()\n",
        "\n",
        "            training_dataset = tf.data.Dataset.from_generator(\n",
        "                lambda: data_generator(train_org_dict, train_forg_dict, batch_size, augment=True, seed=seed+fold),\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32)\n",
        "                )\n",
        "            ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "            validation_dataset = tf.data.Dataset.from_generator(\n",
        "                lambda: data_generator(val_org_dict, val_forg_dict, batch_size, augment=False, seed=seed+fold),\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32)\n",
        "                )\n",
        "            ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "            log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "            lr_scheduler = ReduceLROnPlateau(monitor='val_val_accuracy', factor=0.1, patience=3, mode='max', verbose=1)\n",
        "\n",
        "            checkpoint_filepath = f\"model_checkpoint_fold_{fold}.keras\"\n",
        "            callbacks = [\n",
        "                ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, monitor=\"val_val_accuracy\", mode=\"max\"),\n",
        "                EarlyStopping(monitor=\"val_val_accuracy\", patience=5, mode=\"max\", restore_best_weights=True),\n",
        "                tensorboard_callback\n",
        "            ]\n",
        "\n",
        "            history = model.fit(\n",
        "                training_dataset,\n",
        "                validation_data=validation_dataset,\n",
        "                epochs=epochs,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "                validation_steps=validation_steps,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "\n",
        "            val_accuracy = np.max(history.history['val_val_accuracy'])\n",
        "            all_val_accuracies.append(val_accuracy)\n",
        "            checkpoint_paths.append(checkpoint_filepath)\n",
        "\n",
        "    return all_val_accuracies, checkpoint_paths, history\n"
      ],
      "metadata": {
        "id": "Q0ukRAoA9x7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "-f0VYEiS2Wbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any previous logs to avoid conflicts\n",
        "shutil.rmtree('logs/fit/', ignore_errors=True)"
      ],
      "metadata": {
        "id": "xXV6Ri_xHc7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forg =!unzip -o full_forg.zip"
      ],
      "metadata": {
        "id": "FSuMnHKV2bnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org =!unzip -o full_org.zip"
      ],
      "metadata": {
        "id": "5bznZNn72cP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_dict_test = defaultdict(list)\n",
        "forg_dict_test = defaultdict(list)\n",
        "org_dict_train = defaultdict(list)\n",
        "forg_dict_train = defaultdict(list)\n",
        "\n",
        "process_images(org[1:-1], org_dict_test, org_dict_train)\n",
        "process_images(forg[1:-1], forg_dict_test, forg_dict_train)"
      ],
      "metadata": {
        "id": "uJWpE9Fd2d3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 256\n",
        "margin = 0.2\n",
        "initial_lr = 0.001\n",
        "threshold = 0.8\n",
        "batch_size = 16 * tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
        "all_val_accuracies, checkpoint_paths, history = cross_validate_on_tpu(\n",
        "    org_dict_train,\n",
        "    forg_dict_train,\n",
        "    kf=kf,\n",
        "    batch_size=batch_size,\n",
        "    embedding_size=embedding_size,\n",
        "      epochs=25,\n",
        "    initial_lr=initial_lr,\n",
        "    margin=margin,\n",
        "    threshold=threshold\n",
        ")\n",
        "\n",
        "#%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-CDDXs6C8Gr",
        "outputId": "7293889a-823d-41bf-d741-d93ec1640c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "12/12 [==============================] - 133s 10s/step - loss: 0.1282 - val_val_accuracy: 0.7217\n",
            "Epoch 2/25\n",
            "12/12 [==============================] - 90s 8s/step - loss: 0.1094 - val_val_accuracy: 0.7012\n",
            "Epoch 3/25\n",
            "12/12 [==============================] - 102s 9s/step - loss: 0.1056 - val_val_accuracy: 0.6895\n",
            "Epoch 4/25\n",
            "12/12 [==============================] - 102s 9s/step - loss: 0.1048 - val_val_accuracy: 0.7109\n",
            "Epoch 5/25\n",
            "12/12 [==============================] - 106s 9s/step - loss: 0.1046 - val_val_accuracy: 0.7451\n",
            "Epoch 6/25\n",
            "12/12 [==============================] - 100s 9s/step - loss: 0.1047 - val_val_accuracy: 0.7344\n",
            "Epoch 7/25\n",
            "12/12 [==============================] - 105s 9s/step - loss: 0.1037 - val_val_accuracy: 0.7490\n",
            "Epoch 8/25\n",
            "12/12 [==============================] - 99s 9s/step - loss: 0.1031 - val_val_accuracy: 0.7217\n",
            "Epoch 9/25\n",
            "12/12 [==============================] - 105s 9s/step - loss: 0.1027 - val_val_accuracy: 0.7705\n",
            "Epoch 10/25\n",
            "12/12 [==============================] - 99s 9s/step - loss: 0.1021 - val_val_accuracy: 0.7197\n",
            "Epoch 11/25\n",
            "12/12 [==============================] - 102s 9s/step - loss: 0.1024 - val_val_accuracy: 0.7178\n",
            "Epoch 12/25\n",
            "12/12 [==============================] - 101s 9s/step - loss: 0.1030 - val_val_accuracy: 0.7334\n",
            "Epoch 13/25\n",
            "12/12 [==============================] - 101s 9s/step - loss: 0.1019 - val_val_accuracy: 0.7207\n",
            "Epoch 14/25\n",
            "12/12 [==============================] - 107s 10s/step - loss: 0.1026 - val_val_accuracy: 0.7305\n",
            "Epoch 1/25\n",
            "12/12 [==============================] - 131s 10s/step - loss: 0.1254 - val_val_accuracy: 0.7510\n",
            "Epoch 2/25\n",
            "12/12 [==============================] - 89s 8s/step - loss: 0.1092 - val_val_accuracy: 0.7031\n",
            "Epoch 3/25\n",
            "12/12 [==============================] - 104s 9s/step - loss: 0.1066 - val_val_accuracy: 0.8232\n",
            "Epoch 4/25\n",
            "12/12 [==============================] - 97s 9s/step - loss: 0.1065 - val_val_accuracy: 0.8164\n",
            "Epoch 5/25\n",
            "12/12 [==============================] - 103s 9s/step - loss: 0.1056 - val_val_accuracy: 0.8447\n",
            "Epoch 6/25\n",
            "12/12 [==============================] - 97s 9s/step - loss: 0.1049 - val_val_accuracy: 0.7842\n",
            "Epoch 7/25\n",
            "12/12 [==============================] - 99s 9s/step - loss: 0.1046 - val_val_accuracy: 0.6943\n",
            "Epoch 8/25\n",
            "12/12 [==============================] - 99s 9s/step - loss: 0.1033 - val_val_accuracy: 0.7285\n",
            "Epoch 9/25\n",
            "12/12 [==============================] - 99s 9s/step - loss: 0.1039 - val_val_accuracy: 0.7363\n",
            "Epoch 10/25\n",
            "12/12 [==============================] - 105s 9s/step - loss: 0.1044 - val_val_accuracy: 0.7295\n",
            "Epoch 1/25\n",
            "12/12 [==============================] - 152s 11s/step - loss: 0.1266 - val_val_accuracy: 0.7178\n",
            "Epoch 2/25\n",
            "12/12 [==============================] - 90s 8s/step - loss: 0.1072 - val_val_accuracy: 0.7041\n",
            "Epoch 3/25\n",
            "12/12 [==============================] - 101s 9s/step - loss: 0.1056 - val_val_accuracy: 0.7021\n",
            "Epoch 4/25\n",
            "12/12 [==============================] - 105s 9s/step - loss: 0.1050 - val_val_accuracy: 0.7324\n",
            "Epoch 5/25\n",
            "12/12 [==============================] - 97s 9s/step - loss: 0.1039 - val_val_accuracy: 0.6914\n",
            "Epoch 6/25\n",
            "12/12 [==============================] - 101s 9s/step - loss: 0.1040 - val_val_accuracy: 0.7158\n",
            "Epoch 7/25\n",
            "12/12 [==============================] - 99s 9s/step - loss: 0.1044 - val_val_accuracy: 0.6953\n",
            "Epoch 8/25\n",
            "12/12 [==============================] - 99s 9s/step - loss: 0.1026 - val_val_accuracy: 0.6855\n",
            "Epoch 9/25\n",
            "12/12 [==============================] - 106s 9s/step - loss: 0.1022 - val_val_accuracy: 0.6758\n",
            "Epoch 1/25\n",
            "12/12 [==============================] - 151s 11s/step - loss: 0.1312 - val_val_accuracy: 0.7236\n",
            "Epoch 2/25\n",
            "12/12 [==============================] - 88s 8s/step - loss: 0.1129 - val_val_accuracy: 0.6758\n",
            "Epoch 3/25\n",
            "12/12 [==============================] - 102s 9s/step - loss: 0.1088 - val_val_accuracy: 0.7979\n",
            "Epoch 4/25\n",
            "12/12 [==============================] - 96s 9s/step - loss: 0.1053 - val_val_accuracy: 0.7334\n",
            "Epoch 5/25\n",
            "12/12 [==============================] - 99s 9s/step - loss: 0.1051 - val_val_accuracy: 0.6377\n",
            "Epoch 6/25\n",
            "12/12 [==============================] - 102s 9s/step - loss: 0.1050 - val_val_accuracy: 0.8301\n",
            "Epoch 7/25\n",
            "12/12 [==============================] - 97s 9s/step - loss: 0.1047 - val_val_accuracy: 0.7354\n",
            "Epoch 8/25\n",
            "12/12 [==============================] - 100s 9s/step - loss: 0.1041 - val_val_accuracy: 0.6982\n",
            "Epoch 9/25\n",
            "12/12 [==============================] - 100s 9s/step - loss: 0.1041 - val_val_accuracy: 0.7148\n",
            "Epoch 10/25\n",
            "12/12 [==============================] - 100s 9s/step - loss: 0.1035 - val_val_accuracy: 0.7822\n",
            "Epoch 11/25\n",
            "12/12 [==============================] - 106s 9s/step - loss: 0.1046 - val_val_accuracy: 0.7705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 256\n",
        "margin = 0.2\n",
        "initial_lr = 0.001\n",
        "threshold = 0.9\n",
        "batch_size = 16 * tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "# best_fold_index = np.argmax(all_val_accuracies)\n",
        "# print(best_fold_index)\n",
        "\n",
        "# best_checkpoint_path = checkpoint_paths[best_fold_index]\n",
        "\n",
        "# Instantiate the best model using the same strategy scope\n",
        "with tpu_strategy.scope():\n",
        "    best_model = create_model(embedding_size=embedding_size, margin=margin, initial_lr=initial_lr, threshold=threshold)\n",
        "    best_model.load_weights('model_checkpoint_fold_1.keras')\n",
        "    optimizer = Adam(learning_rate=initial_lr)\n",
        "    best_model.compile(optimizer, weighted_metrics=[])"
      ],
      "metadata": {
        "id": "M3gOHr4t3V2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_test_dataset(org_dict_test, forg_dict_test, batch_size, seed=42):\n",
        "\n",
        "    test_dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: data_generator(org_dict_test, forg_dict_test, batch_size, augment=False, seed=seed),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(batch_size, 220, 155, 3), dtype=tf.float32)\n",
        "        )\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return test_dataset"
      ],
      "metadata": {
        "id": "ICeUF6BsWGdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "best_batch_size = 16 * tpu_strategy.num_replicas_in_sync\n",
        "test_dataset = prepare_test_dataset(org_dict_test, forg_dict_test, best_batch_size)\n",
        "steps = (len(org_dict_test) * 48) // best_batch_size\n",
        "\n",
        "best_model.reset_metrics()\n",
        "evaluation_metrics = best_model.evaluate(test_dataset, steps=steps, return_dict=True)\n",
        "\n",
        "print(\"Evaluation Metrics:\", evaluation_metrics['val_accuracy'])\n",
        "\n",
        "# y_true = evaluation_metrics.pop(\"y_true\", None)\n",
        "# y_pred = evaluation_metrics.pop(\"y_pred\", None)\n",
        "\n",
        "# print(\"Evaluation Metrics:\", evaluation_metrics)\n",
        "# print('')\n",
        "\n",
        "# if y_true is not None and y_pred is not None:\n",
        "#     cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#     plt.figure(figsize=(10, 7))\n",
        "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['True Negative', 'True Positive'])\n",
        "#     plt.xlabel('Predicted')\n",
        "#     plt.ylabel('True')\n",
        "#     plt.title('Confusion Matrix')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "jEAS_UfWijzY",
        "outputId": "0fca15c9-f6f3-4f57-98bf-a1d1e23cb347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics: {'val_accuracy': 0.8388671875}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            
          },
          "metadata": {}
        }
      ]
    }
  ]
}
